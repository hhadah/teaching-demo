---
title: "Linear Regression"
author: "Hussain Hadah"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ["default", "assets/sydney-fonts.css", "assets/sydney.css"]
    self_contained: false # if true, fonts will be stored locally
    seal: true # show a title slide with YAML information
    includes:
      in_header: "assets/mathjax-equation-numbers.html"
    nature:
      beforeInit: ["assets/remark-zoom.js", "https://platform.twitter.com/widgets.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9' # alternatives '16:9' or '4:3' or others e.g. 13:9
      navigation:
        scroll: false # disable slide transitions by scrolling
---
name: toc
layout: true
<div style="position: absolute;left:20px;bottom:5px;color:black;font-size: 12px;">`r rmarkdown::metadata$author` (UH) | `r rmarkdown::metadata$subtitle` | `r format(Sys.time(), '%d %B %Y')`</div>

<!--- `r rmarkdown::metadata$subtitle` | `r format(Sys.time(), '%d %B %Y')`-->

---

```{r, load_refs, echo=FALSE, cache=FALSE, message=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE, 
           bib.style = "authoryear", 
           cite.style = 'authoryear', 
           style = "markdown",
           hyperlink = FALSE, 
           dashed = FALSE)
myBib <- ReadBib("assets/example.bib", check = FALSE)
top_icon = function(x) {
  icons::icon_style(
    icons::fontawesome(x),
    position = "fixed", top = 10, right = 10
  )
}
```

```{r setup, include=FALSE}
# xaringanExtra::use_scribble() ## Draw on slides. Requires dev version of xaringanExtra.

options(htmltools.dir.version = FALSE)
library(knitr)
opts_chunk$set(
  fig.align="center",  
  fig.height=4, fig.width=6,
  out.width="748px", out.length="520.75px",
  dpi=300, #fig.path='Figs/',
  cache=F#, echo=F, warning=F, message=F
  )
```

```{r data, include=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, hrbrthemes, sysfonts, 
               showtext,  gghighlight)

load(file.path("/Users/hhadah/Documents/GiT/teaching-demo/linear-regression","parenthood.Rdata"))

parenthood <- parenthood %>%
  rename(grumpiness = dan.grump,
         hours_slept = dan.sleep,
         dogs = baby.sleep
  )


font_add_google("Fira Sans", "firasans")
font_add_google("Fira Code", "firasans")

showtext_auto()
options(modelsummary_format_numeric_latex = "mathmode")
showtext_opts(dpi = 300)

theme_customs <- function() {
  theme_minimal(base_family = "IBM Plex Sans Condensed") +
    theme(panel.grid.minor = element_blank(),
          plot.background = element_rect(fill = "white", color = NA),
          plot.title = element_text(face = "bold", size = rel(2)),
          axis.title = element_text(face = "bold"),
          strip.text = element_text(face = "bold"),
          strip.background = element_rect(fill = "grey80", color = NA),
          legend.title = element_text(face = "bold", size = rel(1)),
          axis.text.y  = element_text(size = 18),
          axis.text.x  = element_text(size = 18),
          axis.title.x = element_text(size = 20),
          axis.title.y = element_text(size = 20),
          panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.line = element_line(colour = "black"),
          legend.text = element_text(size = rel(1)))
}
```

```{css, echo=F}
    /* Table width = 100% max-width */

    .remark-slide table{
        width: 100%;
    }

    /* Change the background color to white for shaded rows (even rows) */

    .remark-slide thead, .remark-slide tr:nth-child(2n) {
        background-color: white;
    }
    .remark-slide thead, .remark-slide tr:nth-child(n) {
        background-color: white;
    }

    .cell_data--negative {
        content: "-"; /* Add a negative sign before the number */
        margin-right: 2px; /* Add some space between the sign and the number */
        text-decoration: none; /* Remove the underline */
    }
```

## Table of contents

1. [Prologue](#prologue)
2. [Linear Regression](#linear-regression)
4. [Deriving the Regression Equation](#deriving-the-regression-equation)
5. [Ordinary Least Squares](#ols-regression)
5. [Running a Linear Regression in R](#ols-in-r)

---
name: prologue
class: segue-red

# Prologue

---

## What We Will Learn

.blockquote[
### `r icons::fontawesome("location-arrow")` Aim

#### 1. Introduce Linear Regression 

#### 2. Understand the Intuition of Linear Regression 

#### 3. Deriving the Regression Equation

#### 4. Running a Linear Regression in R
]


---
name: linear-regression
class: segue-red

# Linear Regression

---

## Essential parts of regression

.pull-left[
.content-box-purple.large[**Y**]

.content-box-yellow[Outcome variable]

.content-box-yellow[Response variable]

.content-box-yellow[Dependent variable]

.content-box-purple[Thing you want to explain or predict]
]

--

.pull-right[
.content-box-purple.large[**X**]

.content-box-yellow[Explanatory variable]

.content-box-yellow[Predictor variable]

.content-box-yellow[Independent variable]

.content-box-purple[Thing you use to explain or predict **Y**]
]

---
## Identify variables

.pull-left[
.Large[.content-box-purple[
A study examines the effect of smoking on lung cancer
]]

.Large[.content-box-purple[
Researchers predict genocides by looking at negative media coverage, revolutions in neighboring countries, and economic growth
]]
]

.pull-right[
.Large[.content-box-purple[
You want to see if taking more AP classes in high school improves college grades
]]

.Large[.content-box-purple[
Netflix uses your past viewing history, the day of the week, and the time of the day to guess which show you want to watch next
]]
]

---

# Two purposes of regression

.pull-left[
.content-box-purple[Prediction]

.content-box-yellow[Forecast the future]

.content-box-yellow[Focus is on **Y**]

.Large[.content-box-green[
Netflix trying to guess your next show
]]

.Large[.content-box-green[
Predicting who will enroll in SNAP
]]
]

--

.pull-right[
.content-box-purple[Explanation]

.content-box-yellow[Explain effect of **X** on **Y**]

.content-box-yellow[Focus is on **X**]

.Large[.content-box-green[
Netflix looking at the effect of the time of day on show selection
]]

.Large[.content-box-green[
Measuring the effect of SNAP on poverty reduction
]]
]

---
## How Can we do this?

.Large[.content-box-green[
Plot **X** and **Y**
]]

--

.large[.content-box-red[
Draw a line that approximates the relationship
]]

.small[.content-box-red[
and that would plausibly work for data not in the sample!
]]

--

.large[.content-box-red[
Find mathy parts of the line
]]

--

.large[.content-box-red[
Interpret the math
]]

---
## What is a Linear Regression?

.Large[.content-box-purple[
A linear regression is a tool used by economists to analyze relationships between interval scale predictors and interval scale outcomes
]]

--
- What does this definition reminde you of?

--
- Correlations

---
## What is a linear regression model?

```{r regression0, fig.cap="Scatterplot showing grumpiness as a function of hours slept.", echo=FALSE, warning=FALSE}

parenthood |> 
  ggplot(aes(x = hours_slept, y = grumpiness)) +
  geom_point() +
  theme_customs() +
  labs(x = "My sleep (hours)",
       y = "My grumpiness (0-100)")
```

---
count: false
## What is a linear regression model?

```{r regression2, fig.cap="Scatterplot showing grumpiness as a function of hours slept.", echo=FALSE, warning=FALSE, fig.height=3}

parenthood |> 
  ggplot(aes(x = hours_slept, y = grumpiness)) +
  geom_point() +
  theme_customs() +
  theme(axis.title.y = element_text(size = 15)) +
  labs(x = "My sleep (hours)",
       y = "My grumpiness (0-100)")
```

- We can predict my grumpiness based on the data points

---
count: false

## What is a linear regression model?

```{r regression3, fig.cap="Scatterplot showing grumpiness as a function of hours slept.", echo=FALSE, warning=FALSE, fig.height=3}

parenthood |> 
  ggplot(aes(x = hours_slept, y = grumpiness)) +
  geom_point() +
  geom_vline(xintercept = 7, color = "red", linetype = "dashed") +
  gghighlight(hours_slept < 7.1 & hours_slept >  6.9) + 
  theme_customs() +
  theme(axis.title.y = element_text(size = 15)) +
  labs(x = "My sleep (hours)",
       y = "My grumpiness (0-100)")
```

- We can predict my grumpiness based on the data points
- How grumpy could I be if I had 7 hours of sleep?

---
## Predict my grumpiness given any hours of sleep

```{r regression4, fig.cap="Scatterplot showing grumpiness as a function of hours slept.", echo=FALSE, warning=FALSE}

parenthood |> 
  ggplot(aes(x = hours_slept, y = grumpiness)) +
  geom_point() +
  theme_customs() +
  labs(x = "My sleep (hours)",
       y = "My grumpiness (0-100)")
```


---
## Regression: “Best Fitting” Line Through Cloud of Points

```{r regression5, fig.cap="Scatterplot showing grumpiness as a function of hours slept.", echo=FALSE, warning=FALSE, results='hide', message = FALSE}

parenthood |> 
  ggplot(aes(x = hours_slept, y = grumpiness)) +
  geom_point() +
  suppressMessages(geom_smooth(method = "lm", se = FALSE)) +
  theme_customs() +
  labs(x = "My sleep (hours)",
       y = "My grumpiness (0-100)")
```

---
## Regression: “Best Fitting” Line Through Cloud of Points

```{r regression6, fig.cap="Two regression lines.", echo=FALSE, warning=FALSE, results='hide', message = FALSE}

library(gridExtra)
plot1 <- parenthood %>%
  ggplot(aes(x = hours_slept, y = grumpiness)) +
  geom_point() +
  suppressMessages(geom_smooth(method = "lm", se = FALSE)) +
  theme_customs() +
  theme(axis.title.x = element_text(size = 12)) + # Adjust size of x axis title
  theme(axis.title.y = element_text(size = 12)) + # Adjust size of y axis title
  labs(x = "My sleep (hours)",
       y = "My grumpiness (0-100)")

plot2 <- parenthood %>%
  ggplot(aes(x = hours_slept, y = grumpiness)) +
  geom_point() +
  geom_abline(slope = -0.5, intercept = 67) +
  theme_customs() +
  theme(axis.title.x = element_text(size = 12)) + # Adjust size of x axis title
  theme(axis.title.y = element_text(size = 12)) + # Adjust size of y axis title
  labs(x = "My sleep (hours)",
       y = "My grumpiness (0-100)")

grid.arrange(plot1, plot2, ncol = 2)
```

---
name: deriving-the-regression-equation
class: segue-red

# Deriving the Regression Equation

---
## The Regression Equation

#### To derive the regression, let remember some high school math

The formula of a straight line is given by:
$$
y = mx + c
$$
--

- There are two variables, $x$ and $y$

- There are two coefficients, $m$ and $c$

--
- $\pmb{m}$: is the slope of a line and is $y$ divided by the change in $x$

- $\pmb{c}$: is the y-intercept 

--

#### We use the same exact formula to describe a linear regression line:

$$
\hat{Y_i} = \beta_0 + \beta_1 X_i
$$

---
## The Regression Equation (cont.)

$$
\hat{Y_i} = \beta_0 + \beta_1 X_i
$$

- $X_i$: is the value of predictor variable for the _i_th observation
  - numbers of hours of sleep I got on day _i_
- $Y_i$: is the corresponding value of the outcome variable (i.e., my grumpiness on that day)
- $\hat{Y_i}$: is the predicted value of $Y_i$ for a given value of $X_i$
  - $Y_i$ is actual data
  - $\hat{Y_i}$ is the estimate
- $\beta_0$: is the y-intercept
- $\beta_1$: is the slope of the line

---
## The Regression Equation (cont.)

- Not all the data points will fall on the regression line
- In other words, $Y_i$ are not identical to the predicted values from the model $\hat{Y_i}$

--
- The difference between the actual and predicted values is called the **residuals**, $\epsilon_i$:
$$
\epsilon_i = Y_i - \hat{Y_i}
$$

--
- Now we know everything we need to know to write down a linear regression model as:
$$
\hat{Y_i} = \beta_0 + \beta_1 X_i + \epsilon_i
$$

---
name: ols-regression
class: segue-red

# Ordinary Least Squares 

---
## Estimating a Linear Regression Model

- How to we chose the values of $\beta_0$ and $\beta_1$?


```{r regression7, echo=FALSE}
regressionImg <- list()
emphCol <- rgb(0,0,1)
emphColLight <- rgb(.5,.5,1)
emphGrey <- grey(.5)
eps <- TRUE
colour <- TRUE
width <- 6
height <- 6

drawBasicScatterplot <- function(dotcol,title) {
  
    plot( parenthood$hours_slept,
          parenthood$grumpiness,
          xlab = "My sleep (hours)",
          ylab = "My grumpiness (0-100)",
          col= dotcol,
          main = title,
          font.main=1,
          pch=19)
  
  }
good.coef <- lm( grumpiness ~ hours_slept, parenthood)$coef
bad.coef <- c(80,-3)
par(mfrow=c(1,2))
drawBasicScatterplot( emphGrey, " " )
	abline( good.coef, col=ifelse(colour,emphCol,"black"), lwd=3 )
	for(i in seq_along(parenthood$hours_slept)) {
	  xval <- parenthood$hours_slept[i]*c(1,1)
	  yval <- c(parenthood$grumpiness[i],good.coef[1]+good.coef[2]*parenthood$hours_slept[i])
	  lines(xval,yval,type='l', col = emphGrey)
	}
drawBasicScatterplot(emphGrey, " " )
	abline( bad.coef, col=ifelse(colour,emphCol,"black"), lwd=3 )
	for(i in seq_along(parenthood$hours_slept)) {
	  xval <- parenthood$hours_slept[i]*c(1,1)
	  yval <- c(parenthood$grumpiness[i],bad.coef[1]+bad.coef[2]*parenthood$hours_slept[i])
	  lines(xval,yval,type='l', col = emphGrey)
	}

```

---
## Least Squares Regression Predict Using a Line

### The Prediction
Predict my grumpiness $\hat{Y_i} = \beta_0 + \beta_1 X_i$ if I slept $x$ hours

- How to we chose the values of $\beta_0$ and $\beta_1$?

- The lines regression choose the values of $\beta_0$ and $\beta_1$ that minimize the sum of the squared residuals $\sum_{i=1}^n \epsilon_i$

#### The sum of the squared residuals is given by:

$$
\sum_{i=1}^n (Y_i - \hat{Y_i})^2
$$

- When minimized, we will get the optimal estiamted $\hat{\beta_0}$ and $\hat{beta_1}$?

- The more technical name for this estimation process is _.brand-blue[ordinary least squares (OLS) regression]_

---
name: ols-in-r
class: segue-red

# Regression in R

---
## Using the `lm()` function

- The `lm()` function is used to fit linear models in R
- It's a complicated function, type `?lm` to see the documentation
- For this class, we will only use the basic syntax that we've seen before

--
- A `formula`
- A `data` frame

```{r, eval = FALSE}
lm(formula = Y ~ X, data = data.frame)
```
---
## Using the Sleep Data


- First we load the data

```{r, eval = FALSE}
load(file.path("/PATH/TO/DATA","parenthood.Rdata"))
```

- Then we run the regression
- `hours_slept` is the predictor variable or dependent variable
- `grumpiness` is the outcome variable or independent variable

```{r, eval = FALSE}
regression <- lm(formula = grumpiness ~ hours_slept, data = parenthood)
```

- You can print the results using the following options

```{r, eval = FALSE}
print(regression)
summary(regression)
tidy(regression) # broom package
```

---
## Using the Sleep Data (cont.)

```{r, eval = TRUE, include = FALSE}
regression <- lm(formula = grumpiness ~ hours_slept, data = parenthood)
```

- The `print()` function gives us a lot of information about the regression

```{r, eval = TRUE}
print(regression)
```
---
## Using the Sleep Data (cont.)

- The `summary()` function gives us a lot of information about the regression
```{r, eval = TRUE}
summary(regression)
```

---
# Translating Results to Math

$$
\widehat{\text{Grumpiness}} = 125.9563 + (-8.9368) \times \text{Sleep} 
$$

--

- In other words, the estimates that would draw the best fitting line are:

  - $\hat{\beta_0} = 125.9563$, intercept
  - $\hat{\beta_1} = -8.9368$


---
# Interpreting the Regression Results

$$
\widehat{\text{Grumpiness}} = 125.9563 + (8.9368) \times \text{Sleep} 
$$

--
.font80[.content-box-purple[
On average, a one hour increase in sleep is associated with 8.9368 lower Grumpiness, holding everything else constant
]]

--
.font80[.content-box-purple[
The intercept is the expected grumpinness of 125.9563 when sleep is 0 hours `r emo::ji("rage")`
]]

---
# Predicting Grumpiness Given a 5 Hour Sleep

$$
\widehat{\text{Grumpiness}} = 125.9563 + (8.9368) \times \text{Sleep} 
$$

- By hand:

```{r, eval = TRUE}
125.9563 + (-8.9368 * 5)
```

--
- Using `predict()` function

```{r, eval = TRUE}
grumpiness <- data.frame(hours_slept = 5)
predict(regression, newdata = grumpiness)
```

---
## Multiple Linear Regression

- The example we just did was a simple linear regression with a single variable

- Most research projects have multiple predictors

- Out example predicted the effect of sleep on grumpiness

- We could add another predictor, like the number of hours my dogs sleep

### In R

```{r, eval = TRUE}
regression2 <- lm(formula = grumpiness ~ hours_slept + dogs, data = parenthood)
print(regression2)
```
---
## Formula for the General caused

\begin{align*}
Y_i &= \beta_0 + (\sum_{k = 1}^K \beta_k X_{ik}) + \epsilon_i \\
&= \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \dots + \beta_K X_{iK} + \epsilon_i
\end{align*}

---
## How do we Know if a Model is Good?

### The $R^2$

- The $R^2$ is a measure of how well the model fits the data
- It is the proportion of the variance in the outcome variable that is explained by the model

$$
R^2 = \frac{\text{Explained Variation}}{\text{Total Variation}}
$$

--
### We can calculate it using we need to do the following

1. Sum of the squared residuals:
$$
SS_{res} = \sum_i (\epsilon_i)^2 =\sum_i (Y_i - \hat{Y_i})^2
$$

2. Total Sum of Squares:
$$
SS_{tot} = \sum_i (Y_i - \bar{Y_i})^2
$$

---
## In R

```{r, eval = TRUE}
X <- parenthood$hours_slept # predictor
Y <- parenthood$grumpiness # outcome
```

- To calculate $\hat{Y_i}$:

```{r, eval = TRUE}
Y.pred <- -8.94 * X + 125.97
```

--
- The sum squared residuals (SSR):

```{r, eval = TRUE}
SS.res <- sum((Y - Y.pred)^2)
```

--
- The total sum of squares (SST):

```{r, eval = TRUE}
SS.tot <- sum((Y - mean(Y))^2)
paste0("SSR: ", SS.res, " SST: ", SS.tot)
```

---
## In R (cont.)

### The $R^2$

\begin{align*}
R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
\end{align*}

#### Which we can now calculate:


```{r, eval = TRUE}
R.squared <- 1 - (SS.res / SS.tot)
print( R.squared )
```

--

- The $R^2$ is the proportion of the variance in the outcome variable that is explained by the model
- It is sometimes called the .brand-red[coefficient of determination]
- The proportion of the variance in the outcome variable that can be accounted for by the predictor
- So an $R^2$ of 0.82 means that 82% of the variance in my grumpiness (Y) can be explained by my sleep (X)

---
## Relationship Between Regression and Correlations

- As I said at the beggining, regression is similiar to correlation from earlier in the semester

- $r$ is the correlation coefficient

- $R^2$ is identical to $r^2$

```{r, eval = TRUE}
r <- cor(X, Y)
print(r^2)
```

```{r, eval = TRUE} 
R.squared <- 1 - (SS.res / SS.tot)
print( R.squared )
```


```{r gen_pdf, include = FALSE, cache = FALSE, eval = FALSE}
# https://hhadah.github.io/MicroSlides/MyPresentations/intro/intro.html

infile = list.files(path = "/Users/hhadah/Documents/GiT/teaching-demo/index.html", pattern = 'index.html')
pagedown::chrome_print(input = infile, timeout = 100)
xaringan::inf_mr() 
```

